{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import GeonamesDataset\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geonames = GeonamesDataset(\"./data/cities500.txt.gz\", max_len=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sequence</th><th>feature code</th><th>country code</th><th>population</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Oefatu&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;ID&quot;</td><td>0</td></tr><tr><td>&quot;Ballesteros&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;PH&quot;</td><td>5591</td></tr><tr><td>&quot;Korlingen&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;DE&quot;</td><td>788</td></tr><tr><td>&quot;Kushchyovskaya&quot;</td><td>&quot;PPLA2&quot;</td><td>&quot;RU&quot;</td><td>29915</td></tr><tr><td>&quot;Mendlesham&quot;</td><td>&quot;PPL&quot;</td><td>&quot;GB&quot;</td><td>884</td></tr><tr><td>&quot;Empersdorf&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;AT&quot;</td><td>499</td></tr><tr><td>&quot;Witkowo&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;PL&quot;</td><td>7825</td></tr><tr><td>&quot;Atapan&quot;</td><td>&quot;PPL&quot;</td><td>&quot;MX&quot;</td><td>2323</td></tr><tr><td>&quot;Georgenberg&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;DE&quot;</td><td>1507</td></tr><tr><td>&quot;Kanshan&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;CN&quot;</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌────────────────┬──────────────┬──────────────┬────────────┐\n",
       "│ sequence       ┆ feature code ┆ country code ┆ population │\n",
       "│ ---            ┆ ---          ┆ ---          ┆ ---        │\n",
       "│ str            ┆ str          ┆ str          ┆ i64        │\n",
       "╞════════════════╪══════════════╪══════════════╪════════════╡\n",
       "│ Oefatu         ┆ PPLA4        ┆ ID           ┆ 0          │\n",
       "│ Ballesteros    ┆ PPLA3        ┆ PH           ┆ 5591       │\n",
       "│ Korlingen      ┆ PPLA4        ┆ DE           ┆ 788        │\n",
       "│ Kushchyovskaya ┆ PPLA2        ┆ RU           ┆ 29915      │\n",
       "│ Mendlesham     ┆ PPL          ┆ GB           ┆ 884        │\n",
       "│ Empersdorf     ┆ PPLA3        ┆ AT           ┆ 499        │\n",
       "│ Witkowo        ┆ PPLA3        ┆ PL           ┆ 7825       │\n",
       "│ Atapan         ┆ PPL          ┆ MX           ┆ 2323       │\n",
       "│ Georgenberg    ┆ PPLA4        ┆ DE           ┆ 1507       │\n",
       "│ Kanshan        ┆ PPLA4        ┆ CN           ┆ 0          │\n",
       "└────────────────┴──────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geonames.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geonames.df\n",
    "alphabet = \"\".join(\n",
    "    set(\"\".join(df.get_column(\"sequence\").str.split(\"\").explode().to_list()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Tokenizer\n",
    "\n",
    "t = Tokenizer(\n",
    "    alphabet=alphabet,\n",
    "    max_len=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: torch.Size([101200, 16])\n",
      "Test set size: torch.Size([12650, 16])\n",
      "Validation set size: torch.Size([12651, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total_samples = X.size(0)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_train = int(total_samples * train_ratio)\n",
    "num_test = int(total_samples * test_ratio)\n",
    "num_val = total_samples - num_train - num_test\n",
    "\n",
    "# Generate random indices\n",
    "indices = torch.randperm(total_samples)\n",
    "\n",
    "# Split the indices into train, test, and validation sets\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train : num_train + num_test]\n",
    "val_indices = indices[num_train + num_test :]\n",
    "\n",
    "# Create the train, test, and validation sets\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "X_val = X[val_indices]\n",
    "\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Validation set size:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train = TensorDataset(X_train)\n",
    "test = TensorDataset(X_test)\n",
    "val = TensorDataset(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch_ in enumerate(DataLoader(train, batch_size=64)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_T = 100\n",
    "d_embed = 32\n",
    "token_dim = n_tokens = len(t.stoi)\n",
    "d_kq = 12\n",
    "d_hidden = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import ForwardDiffusionProcess, Scheduler\n",
    "\n",
    "from model import TokenDenoiser\n",
    "\n",
    "scheduler = Scheduler(T=max_T)\n",
    "diffusion = ForwardDiffusionProcess(scheduler=scheduler)\n",
    "model = TokenDenoiser(\n",
    "    max_T=max_T,\n",
    "    max_L=token_dim,\n",
    "    d_embed=d_embed,\n",
    "    d_kq=d_kq,\n",
    "    d_hidden=d_hidden,\n",
    "    n_blocks=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2807"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0789d90976d74a7fb2799f4f633f9d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_loss=0.8066, val_loss=0.8066\n",
      "\tp='blocks.0.weight', tensor.grad.sum().item()=0.00018581384210847318\n",
      "\tp='blocks.0.bias', tensor.grad.sum().item()=1.0211981134489179e-05\n",
      "\tp='blocks.1.Q', tensor.grad.sum().item()=-1.4984242397986236e-06\n",
      "\tp='blocks.1.K', tensor.grad.sum().item()=-4.107204176762025e-07\n",
      "\tp='blocks.1.V', tensor.grad.sum().item()=1.5006662579253316e-11\n",
      "\tp='blocks.1.norm.weight', tensor.grad.sum().item()=3.309003488993767e-07\n",
      "\tp='blocks.1.norm.bias', tensor.grad.sum().item()=-1.1856158721457177e-07\n",
      "\tp='blocks.2.Q', tensor.grad.sum().item()=1.3386625141720288e-11\n",
      "\tp='blocks.2.K', tensor.grad.sum().item()=1.3358203432289883e-12\n",
      "\tp='blocks.2.V', tensor.grad.sum().item()=-1.7905676941154525e-12\n",
      "\tp='blocks.2.norm.weight', tensor.grad.sum().item()=1.6777232758613536e-07\n",
      "\tp='blocks.2.norm.bias', tensor.grad.sum().item()=-2.134257215402613e-10\n",
      "\tp='blocks.3.weight', tensor.grad.sum().item()=-2.4374458007514477e-09\n",
      "\tp='blocks.3.bias', tensor.grad.sum().item()=2.091837814077735e-09\n",
      "epoch=1, train_loss=0.8066, val_loss=0.8066\n",
      "\tp='blocks.0.weight', tensor.grad.sum().item()=-6.0412188759073615e-05\n",
      "\tp='blocks.0.bias', tensor.grad.sum().item()=4.879854031969444e-08\n",
      "\tp='blocks.1.Q', tensor.grad.sum().item()=-7.886610546847805e-07\n",
      "\tp='blocks.1.K', tensor.grad.sum().item()=-0.00016810222587082535\n",
      "\tp='blocks.1.V', tensor.grad.sum().item()=-6.586731160496129e-12\n",
      "\tp='blocks.1.norm.weight', tensor.grad.sum().item()=1.055737612887242e-07\n",
      "\tp='blocks.1.norm.bias', tensor.grad.sum().item()=1.2875429433734098e-07\n",
      "\tp='blocks.2.Q', tensor.grad.sum().item()=5.001131739845732e-07\n",
      "\tp='blocks.2.K', tensor.grad.sum().item()=7.610366878907371e-07\n",
      "\tp='blocks.2.V', tensor.grad.sum().item()=-5.077964715383132e-10\n",
      "\tp='blocks.2.norm.weight', tensor.grad.sum().item()=2.2636112362306449e-07\n",
      "\tp='blocks.2.norm.bias', tensor.grad.sum().item()=1.8217143349374965e-09\n",
      "\tp='blocks.3.weight', tensor.grad.sum().item()=1.2406614757765055e-07\n",
      "\tp='blocks.3.bias', tensor.grad.sum().item()=8.349161362275481e-10\n",
      "epoch=2, train_loss=0.8066, val_loss=0.8066\n",
      "\tp='blocks.0.weight', tensor.grad.sum().item()=-1.1810305337867089e-11\n",
      "\tp='blocks.0.bias', tensor.grad.sum().item()=-6.133891138071501e-13\n",
      "\tp='blocks.1.Q', tensor.grad.sum().item()=6.159248879615786e-19\n",
      "\tp='blocks.1.K', tensor.grad.sum().item()=4.880508302718529e-19\n",
      "\tp='blocks.1.V', tensor.grad.sum().item()=3.3306690738754696e-16\n",
      "\tp='blocks.1.norm.weight', tensor.grad.sum().item()=1.2893401901692414e-09\n",
      "\tp='blocks.1.norm.bias', tensor.grad.sum().item()=2.0400283684551823e-09\n",
      "\tp='blocks.2.Q', tensor.grad.sum().item()=0.0\n",
      "\tp='blocks.2.K', tensor.grad.sum().item()=0.0\n",
      "\tp='blocks.2.V', tensor.grad.sum().item()=2.1269108696486683e-10\n",
      "\tp='blocks.2.norm.weight', tensor.grad.sum().item()=6.495617377311191e-09\n",
      "\tp='blocks.2.norm.bias', tensor.grad.sum().item()=-3.138211468467489e-09\n",
      "\tp='blocks.3.weight', tensor.grad.sum().item()=3.289518657467738e-09\n",
      "\tp='blocks.3.bias', tensor.grad.sum().item()=2.535671228542924e-09\n",
      "epoch=3, train_loss=0.8066, val_loss=0.8066\n",
      "\tp='blocks.0.weight', tensor.grad.sum().item()=1.356931789153748e-10\n",
      "\tp='blocks.0.bias', tensor.grad.sum().item()=1.1254865095455813e-11\n",
      "\tp='blocks.1.Q', tensor.grad.sum().item()=0.0\n",
      "\tp='blocks.1.K', tensor.grad.sum().item()=0.0\n",
      "\tp='blocks.1.V', tensor.grad.sum().item()=-7.105427357601002e-15\n",
      "\tp='blocks.1.norm.weight', tensor.grad.sum().item()=4.860114088955925e-09\n",
      "\tp='blocks.1.norm.bias', tensor.grad.sum().item()=-8.670076878658506e-10\n",
      "\tp='blocks.2.Q', tensor.grad.sum().item()=3.044104898708433e-11\n",
      "\tp='blocks.2.K', tensor.grad.sum().item()=1.1525733700779028e-09\n",
      "\tp='blocks.2.V', tensor.grad.sum().item()=-1.1974143898640932e-11\n",
      "\tp='blocks.2.norm.weight', tensor.grad.sum().item()=-3.072536003401183e-09\n",
      "\tp='blocks.2.norm.bias', tensor.grad.sum().item()=-4.922185325995088e-09\n",
      "\tp='blocks.3.weight', tensor.grad.sum().item()=4.752963356224882e-09\n",
      "\tp='blocks.3.bias', tensor.grad.sum().item()=2.548404154367745e-09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(x_0_pred, x_0)\n\u001b[1;32m     34\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# validation mode\u001b[39;00m\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=1.0)\n",
    "\n",
    "# scheduler = lr_scheduler.ExponentialLR(\n",
    "# optimizer,\n",
    "# gamma=0.99,\n",
    "# )\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # get into training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_num in range(X_train.shape[0] // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # initialize proper `x`\n",
    "        ix = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "        batch = X_train[ix].long()\n",
    "\n",
    "        # encode into probas\n",
    "        x_0 = F.one_hot(batch, num_classes=n_tokens).float()\n",
    "        time = torch.randint(low=0, high=max_T, size=(x_0.shape[0],))\n",
    "\n",
    "        x_t = diffusion.sample_T(x_0, time)\n",
    "        x_0_pred = model(batch, time)\n",
    "\n",
    "        loss = F.cross_entropy(x_0_pred, x_0)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "    # validation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        time = torch.randint(0, max_T, size=(X_test.shape[0],))\n",
    "        batch_test = X_test.long()\n",
    "\n",
    "        x_0_test = F.one_hot(batch_test, num_classes=n_tokens).float()\n",
    "        x_t_test = diffusion.sample_T(x_0_test, time)\n",
    "        x_0_test_pred = model(batch_test, time)\n",
    "        test_loss = F.cross_entropy(x_0_test_pred, x_0_test)\n",
    "\n",
    "        print(\n",
    "            f\"{epoch=}, train_loss={torch.tensor(losses).mean().item():2.4f}, val_loss={test_loss.item():2.4f}\"\n",
    "        )\n",
    "\n",
    "        for p, tensor in model.named_parameters():\n",
    "            print(f\"\\t{p=}, {tensor.grad.sum().item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
