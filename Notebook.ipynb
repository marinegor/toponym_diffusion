{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import GeonamesDataset\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "geonames = GeonamesDataset(\"./data/cities500.txt.gz\", max_len=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sequence</th><th>feature code</th><th>country code</th><th>population</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Toutey&quot;</td><td>&quot;PPL&quot;</td><td>&quot;CI&quot;</td><td>996</td></tr><tr><td>&quot;Barbouche&quot;</td><td>&quot;PPL&quot;</td><td>&quot;DZ&quot;</td><td>4897</td></tr><tr><td>&quot;Mulbach&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;DE&quot;</td><td>125</td></tr><tr><td>&quot;Capendu&quot;</td><td>&quot;PPL&quot;</td><td>&quot;FR&quot;</td><td>1525</td></tr><tr><td>&quot;Cantarana&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;IT&quot;</td><td>257</td></tr><tr><td>&quot;Bhusaval&quot;</td><td>&quot;PPL&quot;</td><td>&quot;IN&quot;</td><td>183001</td></tr><tr><td>&quot;Ciguha&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;ID&quot;</td><td>0</td></tr><tr><td>&quot;Rathkeale&quot;</td><td>&quot;PPL&quot;</td><td>&quot;IE&quot;</td><td>1550</td></tr><tr><td>&quot;Geisenhausen&quot;</td><td>&quot;PPL&quot;</td><td>&quot;DE&quot;</td><td>6367</td></tr><tr><td>&quot;Mosses&quot;</td><td>&quot;PPL&quot;</td><td>&quot;US&quot;</td><td>955</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌──────────────┬──────────────┬──────────────┬────────────┐\n",
       "│ sequence     ┆ feature code ┆ country code ┆ population │\n",
       "│ ---          ┆ ---          ┆ ---          ┆ ---        │\n",
       "│ str          ┆ str          ┆ str          ┆ i64        │\n",
       "╞══════════════╪══════════════╪══════════════╪════════════╡\n",
       "│ Toutey       ┆ PPL          ┆ CI           ┆ 996        │\n",
       "│ Barbouche    ┆ PPL          ┆ DZ           ┆ 4897       │\n",
       "│ Mulbach      ┆ PPLA4        ┆ DE           ┆ 125        │\n",
       "│ Capendu      ┆ PPL          ┆ FR           ┆ 1525       │\n",
       "│ Cantarana    ┆ PPLA3        ┆ IT           ┆ 257        │\n",
       "│ Bhusaval     ┆ PPL          ┆ IN           ┆ 183001     │\n",
       "│ Ciguha       ┆ PPLA4        ┆ ID           ┆ 0          │\n",
       "│ Rathkeale    ┆ PPL          ┆ IE           ┆ 1550       │\n",
       "│ Geisenhausen ┆ PPL          ┆ DE           ┆ 6367       │\n",
       "│ Mosses       ┆ PPL          ┆ US           ┆ 955        │\n",
       "└──────────────┴──────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geonames.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geonames.df\n",
    "alphabet = \"\".join(\n",
    "    set(\"\".join(df.get_column(\"sequence\").str.split(\"\").explode().to_list()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Tokenizer\n",
    "\n",
    "t = Tokenizer(\n",
    "    alphabet=alphabet,\n",
    "    max_len=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: torch.Size([101200, 16])\n",
      "Test set size: torch.Size([12650, 16])\n",
      "Validation set size: torch.Size([12651, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total_samples = X.size(0)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_train = int(total_samples * train_ratio)\n",
    "num_test = int(total_samples * test_ratio)\n",
    "num_val = total_samples - num_train - num_test\n",
    "\n",
    "# Generate random indices\n",
    "indices = torch.randperm(total_samples)\n",
    "\n",
    "# Split the indices into train, test, and validation sets\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train : num_train + num_test]\n",
    "val_indices = indices[num_train + num_test :]\n",
    "\n",
    "# Create the train, test, and validation sets\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "X_val = X[val_indices]\n",
    "\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Validation set size:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train = TensorDataset(X_train)\n",
    "test = TensorDataset(X_test)\n",
    "val = TensorDataset(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(DataLoader(train, batch_size=64)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[0].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_output: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_output = d_output\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = self.pe[t].squeeze(-1)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "class MLPWithTime(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_embed: nn.Module,\n",
    "        time_embed: nn.Module,\n",
    "        hidden: nn.Module,\n",
    "        output: nn.Module,\n",
    "        alphabet_size: int,\n",
    "        n_tokens: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed: nn.Module = token_embed\n",
    "        self.time_embed: nn.Module = time_embed\n",
    "        self.hidden: nn.Module = hidden\n",
    "        self.output: nn.Module = output\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.n_tokens = n_tokens\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        te = self.token_embed(x)\n",
    "        pe = self.time_embed(t)\n",
    "        he = self.hidden(te + pe)\n",
    "        output = self.output(he)\n",
    "        return output.reshape(\n",
    "            -1,\n",
    "            self.alphabet_size,\n",
    "            self.n_tokens,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dim = 16\n",
    "word_embeding_dim = time_embeding_dim = 256\n",
    "intermediate_dim = 128\n",
    "hidden_dim = 64\n",
    "dropout = 0.3\n",
    "max_T = 1000\n",
    "alphabet_size = len(t.stoi)\n",
    "\n",
    "Input = nn.Sequential(\n",
    "    nn.Linear(token_dim, word_embeding_dim),\n",
    "    nn.BatchNorm1d(word_embeding_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "TimeInput = PositionalEncoding(\n",
    "    d_model=1,\n",
    "    d_output=time_embeding_dim,\n",
    "    max_len=max_T,\n",
    ")\n",
    "Hidden = nn.Sequential(\n",
    "    nn.Linear(word_embeding_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(intermediate_dim, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(hidden_dim, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(hidden_dim, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(hidden_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    ")\n",
    "Output = nn.Sequential(\n",
    "    nn.Linear(intermediate_dim, alphabet_size * token_dim),\n",
    "    nn.BatchNorm1d(alphabet_size * token_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "\n",
    "model = MLPWithTime(\n",
    "    time_embed=TimeInput,\n",
    "    token_embed=Input,\n",
    "    hidden=Hidden,\n",
    "    output=Output,\n",
    "    alphabet_size=alphabet_size,\n",
    "    n_tokens=token_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = torch.randint(0, max_T, size=(x.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 55, 16])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(x, time).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212368"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import ForwardDiffusionProcess, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = Scheduler(T=max_T)\n",
    "diffusion = ForwardDiffusionProcess(scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=tensor(1.0000e-04), train_loss=4.4510, val_loss=4.0677\n",
      "lr=tensor(0.0001), train_loss=4.4044, val_loss=4.0973\n",
      "lr=tensor(0.0001), train_loss=4.3310, val_loss=4.0879\n",
      "lr=tensor(0.0001), train_loss=4.2429, val_loss=4.0449\n",
      "lr=tensor(0.0002), train_loss=4.1603, val_loss=3.9875\n",
      "lr=tensor(0.0002), train_loss=4.0926, val_loss=3.9216\n",
      "lr=tensor(0.0002), train_loss=4.0293, val_loss=3.8384\n",
      "lr=tensor(0.0002), train_loss=3.9756, val_loss=3.7355\n",
      "lr=tensor(0.0003), train_loss=3.9302, val_loss=3.6811\n",
      "lr=tensor(0.0003), train_loss=3.8918, val_loss=3.6277\n",
      "lr=tensor(0.0003), train_loss=3.8517, val_loss=3.5701\n",
      "lr=tensor(0.0004), train_loss=3.8031, val_loss=3.5599\n",
      "lr=tensor(0.0004), train_loss=3.7571, val_loss=3.5322\n",
      "lr=tensor(0.0005), train_loss=3.7066, val_loss=3.4856\n",
      "lr=tensor(0.0005), train_loss=3.6614, val_loss=3.4833\n",
      "lr=tensor(0.0006), train_loss=3.6277, val_loss=3.4879\n",
      "lr=tensor(0.0006), train_loss=3.6071, val_loss=3.5061\n",
      "lr=tensor(0.0007), train_loss=3.5893, val_loss=3.4919\n",
      "lr=tensor(0.0008), train_loss=3.5690, val_loss=3.4948\n",
      "lr=tensor(0.0009), train_loss=3.5513, val_loss=3.4800\n",
      "lr=tensor(0.0010), train_loss=3.5354, val_loss=3.4647\n",
      "lr=tensor(0.0011), train_loss=3.5068, val_loss=3.4459\n",
      "lr=tensor(0.0013), train_loss=3.4839, val_loss=3.4096\n",
      "lr=tensor(0.0015), train_loss=3.4520, val_loss=3.3652\n",
      "lr=tensor(0.0016), train_loss=3.4223, val_loss=3.3146\n",
      "lr=tensor(0.0018), train_loss=3.3894, val_loss=3.2903\n",
      "lr=tensor(0.0021), train_loss=3.3517, val_loss=3.2332\n",
      "lr=tensor(0.0023), train_loss=3.3140, val_loss=3.1684\n",
      "lr=tensor(0.0026), train_loss=3.2719, val_loss=3.1279\n",
      "lr=tensor(0.0029), train_loss=3.2296, val_loss=3.0850\n",
      "lr=tensor(0.0033), train_loss=3.1887, val_loss=3.0512\n",
      "lr=tensor(0.0037), train_loss=3.1421, val_loss=2.9813\n",
      "lr=tensor(0.0041), train_loss=3.0935, val_loss=2.9257\n",
      "lr=tensor(0.0046), train_loss=3.0406, val_loss=2.8546\n",
      "lr=tensor(0.0052), train_loss=2.9911, val_loss=2.7840\n",
      "lr=tensor(0.0059), train_loss=2.9351, val_loss=2.7195\n",
      "lr=tensor(0.0066), train_loss=2.8733, val_loss=2.6211\n",
      "lr=tensor(0.0074), train_loss=2.8167, val_loss=2.5484\n",
      "lr=tensor(0.0083), train_loss=2.7675, val_loss=2.4641\n",
      "lr=tensor(0.0093), train_loss=2.7154, val_loss=2.3914\n",
      "lr=tensor(0.0105), train_loss=2.6689, val_loss=2.3256\n",
      "lr=tensor(0.0118), train_loss=2.6302, val_loss=2.2378\n",
      "lr=tensor(0.0132), train_loss=2.5928, val_loss=2.1683\n",
      "lr=tensor(0.0148), train_loss=2.5609, val_loss=2.0894\n",
      "lr=tensor(0.0167), train_loss=2.5372, val_loss=2.0319\n",
      "lr=tensor(0.0187), train_loss=2.5171, val_loss=1.9874\n",
      "lr=tensor(0.0210), train_loss=2.5153, val_loss=1.9504\n",
      "lr=tensor(0.0236), train_loss=2.5005, val_loss=1.9150\n",
      "lr=tensor(0.0266), train_loss=2.5051, val_loss=1.9003\n",
      "lr=tensor(0.0298), train_loss=2.4971, val_loss=1.8871\n",
      "lr=tensor(0.0335), train_loss=2.4930, val_loss=1.8841\n",
      "lr=tensor(0.0376), train_loss=2.4837, val_loss=1.8691\n",
      "lr=tensor(0.0423), train_loss=2.4855, val_loss=1.8707\n",
      "lr=tensor(0.0475), train_loss=2.4817, val_loss=1.8604\n",
      "lr=tensor(0.0534), train_loss=2.4846, val_loss=1.8614\n",
      "lr=tensor(0.0599), train_loss=2.4833, val_loss=1.8609\n",
      "lr=tensor(0.0673), train_loss=2.4823, val_loss=1.8452\n",
      "lr=tensor(0.0756), train_loss=2.4810, val_loss=1.8597\n",
      "lr=tensor(0.0850), train_loss=2.4794, val_loss=1.8462\n",
      "lr=tensor(0.0955), train_loss=2.4807, val_loss=1.8443\n",
      "lr=tensor(0.1072), train_loss=2.4834, val_loss=1.8401\n",
      "lr=tensor(0.1205), train_loss=2.4814, val_loss=1.8555\n",
      "lr=tensor(0.1353), train_loss=2.4763, val_loss=1.8396\n",
      "lr=tensor(0.1520), train_loss=2.4844, val_loss=1.8473\n",
      "lr=tensor(0.1707), train_loss=2.4800, val_loss=1.8528\n",
      "lr=tensor(0.1918), train_loss=2.4846, val_loss=1.8445\n",
      "lr=tensor(0.2154), train_loss=2.4872, val_loss=1.8801\n",
      "lr=tensor(0.2420), train_loss=2.4872, val_loss=1.8495\n",
      "lr=tensor(0.2719), train_loss=2.4894, val_loss=1.8595\n",
      "lr=tensor(0.3054), train_loss=2.4911, val_loss=1.8592\n",
      "lr=tensor(0.3430), train_loss=2.4978, val_loss=1.8721\n",
      "lr=tensor(0.3854), train_loss=2.4957, val_loss=1.8765\n",
      "lr=tensor(0.4329), train_loss=2.5025, val_loss=1.9005\n",
      "lr=tensor(0.4863), train_loss=2.5140, val_loss=1.8701\n",
      "lr=tensor(0.5462), train_loss=2.5211, val_loss=1.8789\n",
      "lr=tensor(0.6136), train_loss=2.5312, val_loss=1.9039\n",
      "lr=tensor(0.6893), train_loss=2.5450, val_loss=1.9088\n",
      "lr=tensor(0.7743), train_loss=2.5665, val_loss=1.9179\n",
      "lr=tensor(0.8697), train_loss=2.6042, val_loss=1.9905\n",
      "lr=tensor(0.9770), train_loss=2.6169, val_loss=2.0037\n",
      "lr=tensor(1.0975), train_loss=2.7249, val_loss=2.1426\n",
      "lr=tensor(1.2328), train_loss=2.7255, val_loss=2.2067\n",
      "lr=tensor(1.3849), train_loss=2.7586, val_loss=2.2162\n",
      "lr=tensor(1.5557), train_loss=2.8317, val_loss=2.3495\n",
      "lr=tensor(1.7475), train_loss=2.8744, val_loss=2.4008\n",
      "lr=tensor(1.9630), train_loss=2.9041, val_loss=2.5125\n",
      "lr=tensor(2.2051), train_loss=2.9417, val_loss=48.1896\n",
      "lr=tensor(2.4771), train_loss=2.9929, val_loss=2.5436\n",
      "lr=tensor(2.7826), train_loss=3.0036, val_loss=2.6052\n",
      "lr=tensor(3.1257), train_loss=3.0155, val_loss=56.1806\n",
      "lr=tensor(3.5112), train_loss=3.0530, val_loss=2.6163\n",
      "lr=tensor(3.9442), train_loss=3.0438, val_loss=2.7051\n",
      "lr=tensor(4.4306), train_loss=3.1694, val_loss=2.7522\n",
      "lr=tensor(4.9770), train_loss=3.3020, val_loss=2.9835\n",
      "lr=tensor(5.5908), train_loss=3.2377, val_loss=8.3877\n",
      "lr=tensor(6.2803), train_loss=3.1846, val_loss=2.7455\n",
      "lr=tensor(7.0548), train_loss=3.1550, val_loss=2.7051\n",
      "lr=tensor(7.9248), train_loss=3.4458, val_loss=3.1906\n",
      "lr=tensor(8.9022), train_loss=3.4656, val_loss=3.0979\n",
      "lr=tensor(10.), train_loss=3.3709, val_loss=2.9638\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "batch_size = 4096\n",
    "# num_epochs = 2000\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "# optimizer,\n",
    "# T_max=40,\n",
    "# )\n",
    "\n",
    "for lr in torch.logspace(-4, 1, 100):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr.item())\n",
    "    # training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_num in range(X_train.shape[0] // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ix = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "        batch = X_train[ix].long()\n",
    "        time = torch.randint(low=0, high=max_T, size=(batch.shape[0],))\n",
    "\n",
    "        x_0 = batch.float()\n",
    "        x_t = diffusion.sample_T(x_0, time)\n",
    "        x_0_pred = model(x_t, time)\n",
    "\n",
    "        loss = F.cross_entropy(x_0_pred, batch)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        time = torch.randint(0, max_T, size=(X_test.shape[0],))\n",
    "        x_0_test = X_test.float()\n",
    "        x_t_test = diffusion.sample_T(x_0_test, time)\n",
    "        x_0_test_pred = model(x_t_test, time)\n",
    "        test_loss = F.cross_entropy(x_0_test_pred, X_test.long())\n",
    "\n",
    "        print(\n",
    "            f\"{lr=}, train_loss={torch.tensor(losses).mean().item():2.4f}, val_loss={test_loss.item():2.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4bcb9eec4c4de59a0dd5b789d40bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_loss=3.3057, val_loss=2.6303\n",
      "epoch=1, train_loss=2.7198, val_loss=2.2339\n",
      "epoch=2, train_loss=2.6471, val_loss=2.1931\n",
      "epoch=3, train_loss=2.6134, val_loss=2.1676\n",
      "epoch=4, train_loss=2.5984, val_loss=2.1324\n",
      "epoch=5, train_loss=2.5879, val_loss=2.1117\n",
      "epoch=6, train_loss=2.5809, val_loss=2.0998\n",
      "epoch=7, train_loss=2.5743, val_loss=2.0930\n",
      "epoch=8, train_loss=2.5719, val_loss=2.0774\n",
      "epoch=9, train_loss=2.5667, val_loss=2.0714\n",
      "epoch=10, train_loss=2.5682, val_loss=2.0659\n",
      "epoch=11, train_loss=2.5659, val_loss=2.0648\n",
      "epoch=12, train_loss=2.5639, val_loss=2.0605\n",
      "epoch=13, train_loss=2.5632, val_loss=2.0585\n",
      "epoch=14, train_loss=2.5611, val_loss=2.0566\n",
      "epoch=15, train_loss=2.5639, val_loss=2.0545\n",
      "epoch=16, train_loss=2.5615, val_loss=2.0517\n",
      "epoch=17, train_loss=2.5610, val_loss=2.0532\n",
      "epoch=18, train_loss=2.5617, val_loss=2.0517\n",
      "epoch=19, train_loss=2.5586, val_loss=2.0489\n",
      "epoch=20, train_loss=2.5605, val_loss=2.0496\n",
      "epoch=21, train_loss=2.5598, val_loss=2.0518\n",
      "epoch=22, train_loss=2.5592, val_loss=2.0494\n",
      "epoch=23, train_loss=2.5585, val_loss=2.0492\n",
      "epoch=24, train_loss=2.5597, val_loss=2.0503\n",
      "epoch=25, train_loss=2.5572, val_loss=2.0525\n",
      "epoch=26, train_loss=2.5576, val_loss=2.0497\n",
      "epoch=27, train_loss=2.5593, val_loss=2.0500\n",
      "epoch=28, train_loss=2.5592, val_loss=2.0496\n",
      "epoch=29, train_loss=2.5597, val_loss=2.0499\n",
      "epoch=30, train_loss=2.5607, val_loss=2.0485\n",
      "epoch=31, train_loss=2.5608, val_loss=2.0479\n",
      "epoch=32, train_loss=2.5577, val_loss=2.0495\n",
      "epoch=33, train_loss=2.5584, val_loss=2.0491\n",
      "epoch=34, train_loss=2.5617, val_loss=2.0497\n",
      "epoch=35, train_loss=2.5626, val_loss=2.0498\n",
      "epoch=36, train_loss=2.5619, val_loss=2.0495\n",
      "epoch=37, train_loss=2.5603, val_loss=2.0522\n",
      "epoch=38, train_loss=2.5569, val_loss=2.0491\n",
      "epoch=39, train_loss=2.5588, val_loss=2.0483\n",
      "epoch=40, train_loss=2.5610, val_loss=2.0492\n",
      "epoch=41, train_loss=2.5563, val_loss=2.0493\n",
      "epoch=42, train_loss=2.5578, val_loss=2.0496\n",
      "epoch=43, train_loss=2.5594, val_loss=2.0487\n",
      "epoch=44, train_loss=2.5611, val_loss=2.0496\n",
      "epoch=45, train_loss=2.5589, val_loss=2.0502\n",
      "epoch=46, train_loss=2.5587, val_loss=2.0526\n",
      "epoch=47, train_loss=2.5567, val_loss=2.0519\n",
      "epoch=48, train_loss=2.5609, val_loss=2.0492\n",
      "epoch=49, train_loss=2.5596, val_loss=2.0509\n",
      "epoch=50, train_loss=2.5596, val_loss=2.0499\n",
      "epoch=51, train_loss=2.5587, val_loss=2.0500\n",
      "epoch=52, train_loss=2.5579, val_loss=2.0482\n",
      "epoch=53, train_loss=2.5598, val_loss=2.0495\n",
      "epoch=54, train_loss=2.5565, val_loss=2.0491\n",
      "epoch=55, train_loss=2.5576, val_loss=2.0493\n",
      "epoch=56, train_loss=2.5588, val_loss=2.0494\n",
      "epoch=57, train_loss=2.5582, val_loss=2.0512\n",
      "epoch=58, train_loss=2.5585, val_loss=2.0507\n",
      "epoch=59, train_loss=2.5599, val_loss=2.0529\n",
      "epoch=60, train_loss=2.5585, val_loss=2.0509\n",
      "epoch=61, train_loss=2.5582, val_loss=2.0508\n",
      "epoch=62, train_loss=2.5579, val_loss=2.0498\n",
      "epoch=63, train_loss=2.5584, val_loss=2.0507\n",
      "epoch=64, train_loss=2.5585, val_loss=2.0487\n",
      "epoch=65, train_loss=2.5632, val_loss=2.0499\n",
      "epoch=66, train_loss=2.5593, val_loss=2.0480\n",
      "epoch=67, train_loss=2.5602, val_loss=2.0471\n",
      "epoch=68, train_loss=2.5572, val_loss=2.0479\n",
      "epoch=69, train_loss=2.5589, val_loss=2.0484\n",
      "epoch=70, train_loss=2.5592, val_loss=2.0524\n",
      "epoch=71, train_loss=2.5578, val_loss=2.0505\n",
      "epoch=72, train_loss=2.5606, val_loss=2.0502\n",
      "epoch=73, train_loss=2.5567, val_loss=2.0497\n",
      "epoch=74, train_loss=2.5602, val_loss=2.0495\n",
      "epoch=75, train_loss=2.5623, val_loss=2.0493\n",
      "epoch=76, train_loss=2.5584, val_loss=2.0490\n",
      "epoch=77, train_loss=2.5594, val_loss=2.0511\n",
      "epoch=78, train_loss=2.5616, val_loss=2.0511\n",
      "epoch=79, train_loss=2.5601, val_loss=2.0516\n",
      "epoch=80, train_loss=2.5599, val_loss=2.0516\n",
      "epoch=81, train_loss=2.5595, val_loss=2.0504\n",
      "epoch=82, train_loss=2.5596, val_loss=2.0510\n",
      "epoch=83, train_loss=2.5585, val_loss=2.0499\n",
      "epoch=84, train_loss=2.5617, val_loss=2.0498\n",
      "epoch=85, train_loss=2.5579, val_loss=2.0496\n",
      "epoch=86, train_loss=2.5604, val_loss=2.0487\n",
      "epoch=87, train_loss=2.5571, val_loss=2.0502\n",
      "epoch=88, train_loss=2.5612, val_loss=2.0498\n",
      "epoch=89, train_loss=2.5586, val_loss=2.0518\n",
      "epoch=90, train_loss=2.5577, val_loss=2.0508\n",
      "epoch=91, train_loss=2.5599, val_loss=2.0510\n",
      "epoch=92, train_loss=2.5580, val_loss=2.0522\n",
      "epoch=93, train_loss=2.5589, val_loss=2.0518\n",
      "epoch=94, train_loss=2.5583, val_loss=2.0509\n",
      "epoch=95, train_loss=2.5595, val_loss=2.0518\n",
      "epoch=96, train_loss=2.5594, val_loss=2.0500\n",
      "epoch=97, train_loss=2.5600, val_loss=2.0522\n",
      "epoch=98, train_loss=2.5604, val_loss=2.0487\n",
      "epoch=99, train_loss=2.5590, val_loss=2.0507\n",
      "epoch=100, train_loss=2.5597, val_loss=2.0501\n",
      "epoch=101, train_loss=2.5581, val_loss=2.0514\n",
      "epoch=102, train_loss=2.5582, val_loss=2.0509\n",
      "epoch=103, train_loss=2.5598, val_loss=2.0500\n",
      "epoch=104, train_loss=2.5574, val_loss=2.0499\n",
      "epoch=105, train_loss=2.5585, val_loss=2.0497\n",
      "epoch=106, train_loss=2.5600, val_loss=2.0510\n",
      "epoch=107, train_loss=2.5600, val_loss=2.0503\n",
      "epoch=108, train_loss=2.5619, val_loss=2.0491\n",
      "epoch=109, train_loss=2.5592, val_loss=2.0490\n",
      "epoch=110, train_loss=2.5571, val_loss=2.0508\n",
      "epoch=111, train_loss=2.5589, val_loss=2.0497\n",
      "epoch=112, train_loss=2.5590, val_loss=2.0498\n",
      "epoch=113, train_loss=2.5571, val_loss=2.0499\n",
      "epoch=114, train_loss=2.5575, val_loss=2.0523\n",
      "epoch=115, train_loss=2.5624, val_loss=2.0490\n",
      "epoch=116, train_loss=2.5626, val_loss=2.0501\n",
      "epoch=117, train_loss=2.5585, val_loss=2.0504\n",
      "epoch=118, train_loss=2.5585, val_loss=2.0503\n",
      "epoch=119, train_loss=2.5582, val_loss=2.0485\n",
      "epoch=120, train_loss=2.5594, val_loss=2.0511\n",
      "epoch=121, train_loss=2.5596, val_loss=2.0478\n",
      "epoch=122, train_loss=2.5592, val_loss=2.0488\n",
      "epoch=123, train_loss=2.5628, val_loss=2.0516\n",
      "epoch=124, train_loss=2.5597, val_loss=2.0504\n",
      "epoch=125, train_loss=2.5578, val_loss=2.0500\n",
      "epoch=126, train_loss=2.5582, val_loss=2.0506\n",
      "epoch=127, train_loss=2.5581, val_loss=2.0515\n",
      "epoch=128, train_loss=2.5629, val_loss=2.0506\n",
      "epoch=129, train_loss=2.5573, val_loss=2.0512\n",
      "epoch=130, train_loss=2.5590, val_loss=2.0499\n",
      "epoch=131, train_loss=2.5587, val_loss=2.0515\n",
      "epoch=132, train_loss=2.5563, val_loss=2.0511\n",
      "epoch=133, train_loss=2.5570, val_loss=2.0508\n",
      "epoch=134, train_loss=2.5596, val_loss=2.0505\n",
      "epoch=135, train_loss=2.5592, val_loss=2.0496\n",
      "epoch=136, train_loss=2.5604, val_loss=2.0492\n",
      "epoch=137, train_loss=2.5591, val_loss=2.0512\n",
      "epoch=138, train_loss=2.5612, val_loss=2.0496\n",
      "epoch=139, train_loss=2.5601, val_loss=2.0490\n",
      "epoch=140, train_loss=2.5629, val_loss=2.0500\n",
      "epoch=141, train_loss=2.5589, val_loss=2.0504\n",
      "epoch=142, train_loss=2.5576, val_loss=2.0507\n",
      "epoch=143, train_loss=2.5598, val_loss=2.0511\n",
      "epoch=144, train_loss=2.5597, val_loss=2.0504\n",
      "epoch=145, train_loss=2.5592, val_loss=2.0489\n",
      "epoch=146, train_loss=2.5583, val_loss=2.0497\n",
      "epoch=147, train_loss=2.5607, val_loss=2.0484\n",
      "epoch=148, train_loss=2.5605, val_loss=2.0519\n",
      "epoch=149, train_loss=2.5631, val_loss=2.0495\n",
      "epoch=150, train_loss=2.5585, val_loss=2.0509\n",
      "epoch=151, train_loss=2.5577, val_loss=2.0487\n",
      "epoch=152, train_loss=2.5606, val_loss=2.0494\n",
      "epoch=153, train_loss=2.5582, val_loss=2.0502\n",
      "epoch=154, train_loss=2.5592, val_loss=2.0505\n",
      "epoch=155, train_loss=2.5579, val_loss=2.0491\n",
      "epoch=156, train_loss=2.5598, val_loss=2.0480\n",
      "epoch=157, train_loss=2.5579, val_loss=2.0492\n",
      "epoch=158, train_loss=2.5568, val_loss=2.0488\n",
      "epoch=159, train_loss=2.5597, val_loss=2.0498\n",
      "epoch=160, train_loss=2.5613, val_loss=2.0496\n",
      "epoch=161, train_loss=2.5607, val_loss=2.0495\n",
      "epoch=162, train_loss=2.5596, val_loss=2.0484\n",
      "epoch=163, train_loss=2.5567, val_loss=2.0512\n",
      "epoch=164, train_loss=2.5597, val_loss=2.0499\n",
      "epoch=165, train_loss=2.5572, val_loss=2.0504\n",
      "epoch=166, train_loss=2.5589, val_loss=2.0483\n",
      "epoch=167, train_loss=2.5571, val_loss=2.0508\n",
      "epoch=168, train_loss=2.5589, val_loss=2.0503\n",
      "epoch=169, train_loss=2.5596, val_loss=2.0498\n",
      "epoch=170, train_loss=2.5591, val_loss=2.0502\n",
      "epoch=171, train_loss=2.5599, val_loss=2.0511\n",
      "epoch=172, train_loss=2.5573, val_loss=2.0487\n",
      "epoch=173, train_loss=2.5599, val_loss=2.0517\n",
      "epoch=174, train_loss=2.5608, val_loss=2.0498\n",
      "epoch=175, train_loss=2.5583, val_loss=2.0492\n",
      "epoch=176, train_loss=2.5616, val_loss=2.0502\n",
      "epoch=177, train_loss=2.5591, val_loss=2.0522\n",
      "epoch=178, train_loss=2.5617, val_loss=2.0499\n",
      "epoch=179, train_loss=2.5572, val_loss=2.0518\n",
      "epoch=180, train_loss=2.5603, val_loss=2.0515\n",
      "epoch=181, train_loss=2.5593, val_loss=2.0477\n",
      "epoch=182, train_loss=2.5586, val_loss=2.0481\n",
      "epoch=183, train_loss=2.5631, val_loss=2.0491\n",
      "epoch=184, train_loss=2.5586, val_loss=2.0494\n",
      "epoch=185, train_loss=2.5590, val_loss=2.0515\n",
      "epoch=186, train_loss=2.5608, val_loss=2.0486\n",
      "epoch=187, train_loss=2.5596, val_loss=2.0525\n",
      "epoch=188, train_loss=2.5610, val_loss=2.0490\n",
      "epoch=189, train_loss=2.5616, val_loss=2.0502\n",
      "epoch=190, train_loss=2.5600, val_loss=2.0492\n",
      "epoch=191, train_loss=2.5601, val_loss=2.0484\n",
      "epoch=192, train_loss=2.5620, val_loss=2.0503\n",
      "epoch=193, train_loss=2.5605, val_loss=2.0500\n",
      "epoch=194, train_loss=2.5601, val_loss=2.0487\n",
      "epoch=195, train_loss=2.5612, val_loss=2.0469\n",
      "epoch=196, train_loss=2.5607, val_loss=2.0490\n",
      "epoch=197, train_loss=2.5624, val_loss=2.0477\n",
      "epoch=198, train_loss=2.5606, val_loss=2.0483\n",
      "epoch=199, train_loss=2.5582, val_loss=2.0483\n",
      "epoch=200, train_loss=2.5608, val_loss=2.0500\n",
      "epoch=201, train_loss=2.5585, val_loss=2.0501\n",
      "epoch=202, train_loss=2.5601, val_loss=2.0482\n",
      "epoch=203, train_loss=2.5609, val_loss=2.0513\n",
      "epoch=204, train_loss=2.5612, val_loss=2.0502\n",
      "epoch=205, train_loss=2.5598, val_loss=2.0502\n",
      "epoch=206, train_loss=2.5590, val_loss=2.0497\n",
      "epoch=207, train_loss=2.5616, val_loss=2.0509\n",
      "epoch=208, train_loss=2.5573, val_loss=2.0503\n",
      "epoch=209, train_loss=2.5628, val_loss=2.0506\n",
      "epoch=210, train_loss=2.5607, val_loss=2.0485\n",
      "epoch=211, train_loss=2.5623, val_loss=2.0487\n",
      "epoch=212, train_loss=2.5592, val_loss=2.0519\n",
      "epoch=213, train_loss=2.5565, val_loss=2.0502\n",
      "epoch=214, train_loss=2.5632, val_loss=2.0509\n",
      "epoch=215, train_loss=2.5620, val_loss=2.0498\n",
      "epoch=216, train_loss=2.5595, val_loss=2.0515\n",
      "epoch=217, train_loss=2.5608, val_loss=2.0497\n",
      "epoch=218, train_loss=2.5603, val_loss=2.0508\n",
      "epoch=219, train_loss=2.5561, val_loss=2.0512\n",
      "epoch=220, train_loss=2.5583, val_loss=2.0491\n",
      "epoch=221, train_loss=2.5587, val_loss=2.0483\n",
      "epoch=222, train_loss=2.5602, val_loss=2.0498\n",
      "epoch=223, train_loss=2.5613, val_loss=2.0483\n",
      "epoch=224, train_loss=2.5612, val_loss=2.0491\n",
      "epoch=225, train_loss=2.5579, val_loss=2.0522\n",
      "epoch=226, train_loss=2.5582, val_loss=2.0500\n",
      "epoch=227, train_loss=2.5581, val_loss=2.0507\n",
      "epoch=228, train_loss=2.5593, val_loss=2.0485\n",
      "epoch=229, train_loss=2.5613, val_loss=2.0494\n",
      "epoch=230, train_loss=2.5570, val_loss=2.0514\n",
      "epoch=231, train_loss=2.5589, val_loss=2.0492\n",
      "epoch=232, train_loss=2.5586, val_loss=2.0492\n",
      "epoch=233, train_loss=2.5577, val_loss=2.0488\n",
      "epoch=234, train_loss=2.5599, val_loss=2.0504\n",
      "epoch=235, train_loss=2.5596, val_loss=2.0511\n",
      "epoch=236, train_loss=2.5597, val_loss=2.0502\n",
      "epoch=237, train_loss=2.5584, val_loss=2.0504\n",
      "epoch=238, train_loss=2.5590, val_loss=2.0510\n",
      "epoch=239, train_loss=2.5571, val_loss=2.0495\n",
      "epoch=240, train_loss=2.5578, val_loss=2.0513\n",
      "epoch=241, train_loss=2.5603, val_loss=2.0489\n",
      "epoch=242, train_loss=2.5606, val_loss=2.0492\n",
      "epoch=243, train_loss=2.5614, val_loss=2.0512\n",
      "epoch=244, train_loss=2.5636, val_loss=2.0496\n",
      "epoch=245, train_loss=2.5586, val_loss=2.0487\n",
      "epoch=246, train_loss=2.5600, val_loss=2.0519\n",
      "epoch=247, train_loss=2.5618, val_loss=2.0496\n",
      "epoch=248, train_loss=2.5574, val_loss=2.0500\n",
      "epoch=249, train_loss=2.5610, val_loss=2.0520\n",
      "epoch=250, train_loss=2.5591, val_loss=2.0500\n",
      "epoch=251, train_loss=2.5595, val_loss=2.0487\n",
      "epoch=252, train_loss=2.5601, val_loss=2.0503\n",
      "epoch=253, train_loss=2.5600, val_loss=2.0500\n",
      "epoch=254, train_loss=2.5600, val_loss=2.0496\n",
      "epoch=255, train_loss=2.5596, val_loss=2.0505\n",
      "epoch=256, train_loss=2.5595, val_loss=2.0512\n",
      "epoch=257, train_loss=2.5592, val_loss=2.0506\n",
      "epoch=258, train_loss=2.5614, val_loss=2.0514\n",
      "epoch=259, train_loss=2.5621, val_loss=2.0490\n",
      "epoch=260, train_loss=2.5612, val_loss=2.0502\n",
      "epoch=261, train_loss=2.5582, val_loss=2.0512\n",
      "epoch=262, train_loss=2.5568, val_loss=2.0517\n",
      "epoch=263, train_loss=2.5594, val_loss=2.0507\n",
      "epoch=264, train_loss=2.5605, val_loss=2.0490\n",
      "epoch=265, train_loss=2.5595, val_loss=2.0506\n",
      "epoch=266, train_loss=2.5612, val_loss=2.0504\n",
      "epoch=267, train_loss=2.5572, val_loss=2.0494\n",
      "epoch=268, train_loss=2.5615, val_loss=2.0508\n",
      "epoch=269, train_loss=2.5571, val_loss=2.0503\n",
      "epoch=270, train_loss=2.5622, val_loss=2.0498\n",
      "epoch=271, train_loss=2.5605, val_loss=2.0499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     28\u001b[0m x_t \u001b[38;5;241m=\u001b[39m diffusion\u001b[38;5;241m.\u001b[39msample_T(x_0, time)\n\u001b[0;32m---> 29\u001b[0m x_0_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(x_0_pred, batch)\n\u001b[1;32m     32\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[247], line 27\u001b[0m, in \u001b[0;36mMLPWithTime.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     26\u001b[0m     te \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embed(x)\n\u001b[0;32m---> 27\u001b[0m     pe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     he \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden(te \u001b[38;5;241m+\u001b[39m pe)\n\u001b[1;32m     29\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(he)\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[246], line 29\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m        x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "batch_size = 4096\n",
    "num_epochs = 2000\n",
    "optimizer = AdamW(model.parameters(), lr=0.1)\n",
    "\n",
    "scheduler = lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_num in range(X_train.shape[0] // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ix = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "        batch = X_train[ix].long()\n",
    "        time = torch.randint(low=0, high=max_T, size=(batch.shape[0],))\n",
    "\n",
    "        x_0 = batch.float()\n",
    "        x_t = diffusion.sample_T(x_0, time)\n",
    "        x_0_pred = model(x_t, time)\n",
    "\n",
    "        loss = F.cross_entropy(x_0_pred, batch)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # validation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        time = torch.randint(0, max_T, size=(X_test.shape[0],))\n",
    "        x_0_test = X_test.float()\n",
    "        x_t_test = diffusion.sample_T(x_0_test, time)\n",
    "        x_0_test_pred = model(x_t_test, time)\n",
    "        test_loss = F.cross_entropy(x_0_test_pred, X_test.long())\n",
    "\n",
    "        print(\n",
    "            f\"{epoch=}, train_loss={torch.tensor(losses).mean().item():2.4f}, val_loss={test_loss.item():2.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import ForwardDiffusionProcess\n",
    "scheduler = Scheduler(T=max_T)\n",
    "diffusion = ForwardDiffusionProcess(scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = diffusion.sample(model=model, n=10, max_T=max_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<Soelhreein><>><',\n",
       " '<Saneeeaaea>>...',\n",
       " '<Sanaeeaaea>>...',\n",
       " '<Woelhreein><>><',\n",
       " '<Soelhreein><>><',\n",
       " '<Soelhreein><>><',\n",
       " '<Soelhreein><>><',\n",
       " '<Saneeeaaea>>...',\n",
       " '<Saneeeaaea>>...',\n",
       " '<Sanaenaae>>....']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode_raw(denoised.argmax(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
