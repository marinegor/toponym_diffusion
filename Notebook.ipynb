{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import GeonamesDataset\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geonames = GeonamesDataset(\"./data/cities500.txt.gz\", max_len=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sequence</th><th>feature code</th><th>country code</th><th>population</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Mislesevo&quot;</td><td>&quot;PPL&quot;</td><td>&quot;MK&quot;</td><td>1988</td></tr><tr><td>&quot;Paracin&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;RS&quot;</td><td>6000</td></tr><tr><td>&quot;Mekarsari&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;ID&quot;</td><td>0</td></tr><tr><td>&quot;Waterville&quot;</td><td>&quot;PPL&quot;</td><td>&quot;US&quot;</td><td>651</td></tr><tr><td>&quot;Kistler&quot;</td><td>&quot;PPL&quot;</td><td>&quot;US&quot;</td><td>528</td></tr><tr><td>&quot;Suzemka&quot;</td><td>&quot;PPL&quot;</td><td>&quot;RU&quot;</td><td>9394</td></tr><tr><td>&quot;Mladenovac&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;RS&quot;</td><td>0</td></tr><tr><td>&quot;Dalizi&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;CN&quot;</td><td>0</td></tr><tr><td>&quot;Krasnany&quot;</td><td>&quot;PPL&quot;</td><td>&quot;SK&quot;</td><td>1231</td></tr><tr><td>&quot;Pomona&quot;</td><td>&quot;PPL&quot;</td><td>&quot;US&quot;</td><td>153266</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌────────────┬──────────────┬──────────────┬────────────┐\n",
       "│ sequence   ┆ feature code ┆ country code ┆ population │\n",
       "│ ---        ┆ ---          ┆ ---          ┆ ---        │\n",
       "│ str        ┆ str          ┆ str          ┆ i64        │\n",
       "╞════════════╪══════════════╪══════════════╪════════════╡\n",
       "│ Mislesevo  ┆ PPL          ┆ MK           ┆ 1988       │\n",
       "│ Paracin    ┆ PPLA3        ┆ RS           ┆ 6000       │\n",
       "│ Mekarsari  ┆ PPLA4        ┆ ID           ┆ 0          │\n",
       "│ Waterville ┆ PPL          ┆ US           ┆ 651        │\n",
       "│ Kistler    ┆ PPL          ┆ US           ┆ 528        │\n",
       "│ Suzemka    ┆ PPL          ┆ RU           ┆ 9394       │\n",
       "│ Mladenovac ┆ PPLA3        ┆ RS           ┆ 0          │\n",
       "│ Dalizi     ┆ PPLA4        ┆ CN           ┆ 0          │\n",
       "│ Krasnany   ┆ PPL          ┆ SK           ┆ 1231       │\n",
       "│ Pomona     ┆ PPL          ┆ US           ┆ 153266     │\n",
       "└────────────┴──────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geonames.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geonames.df\n",
    "alphabet = \"\".join(\n",
    "    set(\"\".join(df.get_column(\"sequence\").str.split(\"\").explode().to_list()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Tokenizer\n",
    "\n",
    "t = Tokenizer(\n",
    "    alphabet=alphabet,\n",
    "    max_len=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: torch.Size([101200, 16])\n",
      "Test set size: torch.Size([12650, 16])\n",
      "Validation set size: torch.Size([12651, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total_samples = X.size(0)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_train = int(total_samples * train_ratio)\n",
    "num_test = int(total_samples * test_ratio)\n",
    "num_val = total_samples - num_train - num_test\n",
    "\n",
    "# Generate random indices\n",
    "indices = torch.randperm(total_samples)\n",
    "\n",
    "# Split the indices into train, test, and validation sets\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train : num_train + num_test]\n",
    "val_indices = indices[num_train + num_test :]\n",
    "\n",
    "# Create the train, test, and validation sets\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "X_val = X[val_indices]\n",
    "\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Validation set size:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train = TensorDataset(X_train)\n",
    "test = TensorDataset(X_test)\n",
    "val = TensorDataset(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(DataLoader(train, batch_size=64)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[0].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_output: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_output = d_output\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = self.pe[t].squeeze(-1)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "class MLPWithTime(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_embed: nn.Module,\n",
    "        time_embed: nn.Module,\n",
    "        hidden: nn.Module,\n",
    "        output: nn.Module,\n",
    "        alphabet_size: int,\n",
    "        n_tokens: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed: nn.Module = token_embed\n",
    "        self.time_embed: nn.Module = time_embed\n",
    "        self.hidden: nn.Module = hidden\n",
    "        self.output: nn.Module = output\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.n_tokens = n_tokens\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        te = self.token_embed(x)\n",
    "        pe = self.time_embed(t)\n",
    "        he = self.hidden(te + pe)\n",
    "        output = self.output(he)\n",
    "        return output.reshape(\n",
    "            -1,\n",
    "            self.alphabet_size,\n",
    "            self.n_tokens,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dim = 16\n",
    "word_embeding_dim = time_embeding_dim = 256\n",
    "intermediate_dim = 128\n",
    "hidden_dim = 64\n",
    "dropout = 0.3\n",
    "max_T = 1000\n",
    "alphabet_size = len(t.stoi)\n",
    "\n",
    "Input = nn.Sequential(\n",
    "    nn.Linear(token_dim, word_embeding_dim),\n",
    "    nn.BatchNorm1d(word_embeding_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "TimeInput = PositionalEncoding(\n",
    "    d_model=1,\n",
    "    d_output=time_embeding_dim,\n",
    "    max_len=max_T,\n",
    ")\n",
    "Hidden = nn.Sequential(\n",
    "    nn.Linear(word_embeding_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(intermediate_dim, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(hidden_dim, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(hidden_dim, hidden_dim),\n",
    "    nn.BatchNorm1d(hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(hidden_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    # ----\n",
    ")\n",
    "Output = nn.Sequential(\n",
    "    nn.Linear(intermediate_dim, alphabet_size * token_dim),\n",
    "    nn.BatchNorm1d(alphabet_size * token_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "\n",
    "model = MLPWithTime(\n",
    "    time_embed=TimeInput,\n",
    "    token_embed=Input,\n",
    "    hidden=Hidden,\n",
    "    output=Output,\n",
    "    alphabet_size=alphabet_size,\n",
    "    n_tokens=token_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = torch.randint(0, max_T, size=(x.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 55, 16])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(x, time).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212368"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import ForwardDiffusionProcess, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = Scheduler(T=max_T)\n",
    "diffusion = ForwardDiffusionProcess(scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], size\u001b[38;5;241m=\u001b[39m(batch_size,))\n\u001b[0;32m---> 22\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m time \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mmax_T, size\u001b[38;5;241m=\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],))\n\u001b[1;32m     25\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "batch_size = 4096\n",
    "# num_epochs = 2000\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "# optimizer,\n",
    "# T_max=40,\n",
    "# )\n",
    "\n",
    "for lr in torch.logspace(-4, 1, 100):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr.item())\n",
    "    # training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_num in range(X_train.shape[0] // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ix = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "        batch = X_train[ix].long()\n",
    "        time = torch.randint(low=0, high=max_T, size=(batch.shape[0],))\n",
    "\n",
    "        x_0 = batch.float()\n",
    "        x_t = diffusion.sample_T(x_0, time)\n",
    "        x_0_pred = model(x_t, time)\n",
    "\n",
    "        loss = F.cross_entropy(x_0_pred, batch)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        time = torch.randint(0, max_T, size=(X_test.shape[0],))\n",
    "        x_0_test = X_test.float()\n",
    "        x_t_test = diffusion.sample_T(x_0_test, time)\n",
    "        x_0_test_pred = model(x_t_test, time)\n",
    "        test_loss = F.cross_entropy(x_0_test_pred, X_test.long())\n",
    "\n",
    "        print(\n",
    "            f\"{lr=}, train_loss={torch.tensor(losses).mean().item():2.4f}, val_loss={test_loss.item():2.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ee7f4cc07c4a17a78a5be3722bf700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0_pred.shape=torch.Size([4096, 55, 16])\n",
      "batch.shape=torch.Size([4096, 16])\n",
      "x_0_pred.shape=torch.Size([4096, 55, 16])\n",
      "batch.shape=torch.Size([4096, 16])\n",
      "x_0_pred.shape=torch.Size([4096, 55, 16])\n",
      "batch.shape=torch.Size([4096, 16])\n",
      "x_0_pred.shape=torch.Size([4096, 55, 16])\n",
      "batch.shape=torch.Size([4096, 16])\n",
      "x_0_pred.shape=torch.Size([4096, 55, 16])\n",
      "batch.shape=torch.Size([4096, 16])\n",
      "x_0_pred.shape=torch.Size([4096, 55, 16])\n",
      "batch.shape=torch.Size([4096, 16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(x_0_pred, batch)\n\u001b[1;32m     34\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/toponym_diffusion/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "batch_size = 4096\n",
    "num_epochs = 2000\n",
    "optimizer = AdamW(model.parameters(), lr=0.1)\n",
    "\n",
    "scheduler = lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_num in range(X_train.shape[0] // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ix = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "        batch = X_train[ix].long()\n",
    "        time = torch.randint(low=0, high=max_T, size=(batch.shape[0],))\n",
    "\n",
    "        x_0 = batch.float()\n",
    "        x_t = diffusion.sample_T(x_0, time)\n",
    "        x_0_pred = model(x_t, time)\n",
    "\n",
    "        print(f'{x_0_pred.shape=}')\n",
    "        print(f'{batch.shape=}')\n",
    "        loss = F.cross_entropy(x_0_pred, batch)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # validation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        time = torch.randint(0, max_T, size=(X_test.shape[0],))\n",
    "        x_0_test = X_test.float()\n",
    "        x_t_test = diffusion.sample_T(x_0_test, time)\n",
    "        x_0_test_pred = model(x_t_test, time)\n",
    "        print(f'{x_0_test_pred.shape=}')\n",
    "        print(f'{X_test.shape=}')\n",
    "        test_loss = F.cross_entropy(x_0_test_pred, X_test.long())\n",
    "\n",
    "        print(\n",
    "            f\"{epoch=}, train_loss={torch.tensor(losses).mean().item():2.4f}, val_loss={test_loss.item():2.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import ForwardDiffusionProcess\n",
    "scheduler = Scheduler(T=max_T)\n",
    "diffusion = ForwardDiffusionProcess(scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = diffusion.sample(model=model, n=10, max_T=max_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<Soelhreein><>><',\n",
       " '<Saneeeaaea>>...',\n",
       " '<Sanaeeaaea>>...',\n",
       " '<Woelhreein><>><',\n",
       " '<Soelhreein><>><',\n",
       " '<Soelhreein><>><',\n",
       " '<Soelhreein><>><',\n",
       " '<Saneeeaaea>>...',\n",
       " '<Saneeeaaea>>...',\n",
       " '<Sanaenaae>>....']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode_raw(denoised.argmax(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
