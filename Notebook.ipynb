{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import GeonamesDataset\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "geonames = GeonamesDataset(\"./data/cities500.txt.gz\", max_len=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sequence</th><th>feature code</th><th>country code</th><th>population</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Badarganj&quot;</td><td>&quot;PPL&quot;</td><td>&quot;BD&quot;</td><td>32600</td></tr><tr><td>&quot;Songmai&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;CN&quot;</td><td>0</td></tr><tr><td>&quot;Fenghua&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;CN&quot;</td><td>76653</td></tr><tr><td>&quot;Dschang&quot;</td><td>&quot;PPL&quot;</td><td>&quot;CM&quot;</td><td>99582</td></tr><tr><td>&quot;Demirtas&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;TR&quot;</td><td>6702</td></tr><tr><td>&quot;Komyshuvakha&quot;</td><td>&quot;PPL&quot;</td><td>&quot;UA&quot;</td><td>5211</td></tr><tr><td>&quot;Bareqet&quot;</td><td>&quot;PPL&quot;</td><td>&quot;IL&quot;</td><td>2082</td></tr><tr><td>&quot;Yerevan&quot;</td><td>&quot;PPLC&quot;</td><td>&quot;AM&quot;</td><td>1093485</td></tr><tr><td>&quot;Kilmacow&quot;</td><td>&quot;PPL&quot;</td><td>&quot;IE&quot;</td><td>647</td></tr><tr><td>&quot;Antonimina&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;IT&quot;</td><td>506</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌──────────────┬──────────────┬──────────────┬────────────┐\n",
       "│ sequence     ┆ feature code ┆ country code ┆ population │\n",
       "│ ---          ┆ ---          ┆ ---          ┆ ---        │\n",
       "│ str          ┆ str          ┆ str          ┆ i64        │\n",
       "╞══════════════╪══════════════╪══════════════╪════════════╡\n",
       "│ Badarganj    ┆ PPL          ┆ BD           ┆ 32600      │\n",
       "│ Songmai      ┆ PPLA3        ┆ CN           ┆ 0          │\n",
       "│ Fenghua      ┆ PPLA3        ┆ CN           ┆ 76653      │\n",
       "│ Dschang      ┆ PPL          ┆ CM           ┆ 99582      │\n",
       "│ Demirtas     ┆ PPLA3        ┆ TR           ┆ 6702       │\n",
       "│ Komyshuvakha ┆ PPL          ┆ UA           ┆ 5211       │\n",
       "│ Bareqet      ┆ PPL          ┆ IL           ┆ 2082       │\n",
       "│ Yerevan      ┆ PPLC         ┆ AM           ┆ 1093485    │\n",
       "│ Kilmacow     ┆ PPL          ┆ IE           ┆ 647        │\n",
       "│ Antonimina   ┆ PPLA3        ┆ IT           ┆ 506        │\n",
       "└──────────────┴──────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geonames.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geonames.df\n",
    "alphabet = \"\".join(\n",
    "    set(\"\".join(df.get_column(\"sequence\").str.split(\"\").explode().to_list()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Tokenizer\n",
    "\n",
    "t = Tokenizer(\n",
    "    alphabet=alphabet,\n",
    "    max_len=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: torch.Size([101200, 16])\n",
      "Test set size: torch.Size([12650, 16])\n",
      "Validation set size: torch.Size([12651, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total_samples = X.size(0)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_train = int(total_samples * train_ratio)\n",
    "num_test = int(total_samples * test_ratio)\n",
    "num_val = total_samples - num_train - num_test\n",
    "\n",
    "# Generate random indices\n",
    "indices = torch.randperm(total_samples)\n",
    "\n",
    "# Split the indices into train, test, and validation sets\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train : num_train + num_test]\n",
    "val_indices = indices[num_train + num_test :]\n",
    "\n",
    "# Create the train, test, and validation sets\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "X_val = X[val_indices]\n",
    "\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Validation set size:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train = TensorDataset(X_train)\n",
    "test = TensorDataset(X_test)\n",
    "val = TensorDataset(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization\n",
    "\n",
    "Let's train a model that would predict a single missing letter in the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtyping import TensorType\n",
    "\n",
    "T = TensorType\n",
    "\n",
    "\n",
    "class RandomMasker:\n",
    "    def __init__(self, tokenizer: Tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_token: int = tokenizer.stoi[tokenizer.start_token]\n",
    "        self.end_token: int = tokenizer.stoi[tokenizer.end_token]\n",
    "        self.pad_token: int = tokenizer.stoi[tokenizer.pad_token]\n",
    "        self.mask_token: int = tokenizer.stoi[tokenizer.mask_token]\n",
    "\n",
    "    def add_mask(self, x: T[\"b\", \"max_L\", torch.long], p: float = 0.1):  # noqa: F821\n",
    "        where = (x != self.start_token) & (x != self.end_token) & (x != self.pad_token)\n",
    "        mask = (torch.randint_like(where.long(), low=0, high=100) < p * 100) & where\n",
    "        x[mask] = self.mask_token\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from model import PositionalEncoding\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        n_tokens: int,\n",
    "        hidden_dim: int,\n",
    "        dropout: float = 0.4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_tokens = n_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.pe = PositionalEncoding(d_embed=embed_dim, max_L=n_tokens)\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm((n_tokens, hidden_dim)),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm((n_tokens, hidden_dim)),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: T[\"b\", \"max_L\"]):  # noqa: F821\n",
    "        xe = self.emb(x)\n",
    "        pe = self.pe(x)\n",
    "        xe += pe\n",
    "        return self.layers(xe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27320"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "vocab_size = len(t.stoi)\n",
    "n_tokens = t.max_len\n",
    "hidden_dim = 64\n",
    "\n",
    "model = Model(\n",
    "    embed_dim=embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    n_tokens=n_tokens,\n",
    "    hidden_dim=hidden_dim,\n",
    ")\n",
    "\n",
    "sum((p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (pe): PositionalEncoding()\n",
       "   (emb): Embedding(56, 128)\n",
       "   (layers): Sequential(\n",
       "     (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Dropout(p=0.4, inplace=False)\n",
       "     (3): LayerNorm((16, 64), eps=1e-05, elementwise_affine=True)\n",
       "     (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Dropout(p=0.4, inplace=False)\n",
       "     (7): LayerNorm((16, 64), eps=1e-05, elementwise_affine=True)\n",
       "     (8): Linear(in_features=64, out_features=56, bias=True)\n",
       "     (9): ReLU()\n",
       "   )\n",
       " ),\n",
       " tensor([[ 1, 19, 44,  ...,  3,  3,  3],\n",
       "         [ 1, 23,  9,  ...,  3,  3,  3],\n",
       "         [ 1, 25, 13,  ...,  3,  3,  3],\n",
       "         ...,\n",
       "         [ 1, 31, 13,  ...,  3,  3,  3],\n",
       "         [ 1, 23, 39,  ...,  3,  3,  3],\n",
       "         [ 1, 46, 13,  ...,  3,  3,  3]], device='cuda:0', dtype=torch.uint8),\n",
       " tensor([[ 1, 33, 39,  ...,  3,  3,  3],\n",
       "         [ 1, 19, 38,  ...,  3,  3,  3],\n",
       "         [ 1, 12, 13,  ...,  3,  3,  3],\n",
       "         ...,\n",
       "         [ 1, 52, 42,  ...,  3,  3,  3],\n",
       "         [ 1, 31, 39,  ...,  3,  3,  3],\n",
       "         [ 1, 31, 13,  ...,  3,  3,  3]], device='cuda:0', dtype=torch.uint8))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\"), X_train.to(\"cuda\"), X_test.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2723a4d81d445f9a275e63aa6dd6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, train_loss=4.0291, val_loss=2.3900\n",
      "epoch=10, train_loss=1.5841, val_loss=1.2169\n",
      "epoch=20, train_loss=0.8291, val_loss=0.5130\n",
      "epoch=30, train_loss=0.4642, val_loss=0.2159\n",
      "epoch=40, train_loss=0.2525, val_loss=0.0474\n",
      "epoch=50, train_loss=0.1551, val_loss=0.0291\n",
      "epoch=60, train_loss=0.1086, val_loss=0.0188\n",
      "epoch=70, train_loss=0.0844, val_loss=0.0156\n",
      "epoch=80, train_loss=0.0702, val_loss=0.0134\n",
      "epoch=90, train_loss=0.0618, val_loss=0.0151\n",
      "epoch=100, train_loss=0.0551, val_loss=0.0132\n",
      "epoch=110, train_loss=0.0508, val_loss=0.0138\n",
      "epoch=120, train_loss=0.0474, val_loss=0.0138\n",
      "epoch=130, train_loss=0.0442, val_loss=0.0135\n",
      "epoch=140, train_loss=0.0422, val_loss=0.0130\n",
      "epoch=150, train_loss=0.0400, val_loss=0.0129\n",
      "epoch=160, train_loss=0.0390, val_loss=0.0128\n",
      "epoch=170, train_loss=0.0375, val_loss=0.0118\n",
      "epoch=180, train_loss=0.0370, val_loss=0.0119\n",
      "epoch=190, train_loss=0.0351, val_loss=0.0121\n",
      "epoch=200, train_loss=0.0346, val_loss=0.0121\n",
      "epoch=210, train_loss=0.0343, val_loss=0.0119\n",
      "epoch=220, train_loss=0.0327, val_loss=0.0122\n",
      "epoch=230, train_loss=0.0328, val_loss=0.0119\n",
      "epoch=240, train_loss=0.0320, val_loss=0.0115\n",
      "epoch=250, train_loss=0.0315, val_loss=0.0117\n",
      "epoch=260, train_loss=0.0319, val_loss=0.0108\n",
      "epoch=270, train_loss=0.0310, val_loss=0.0115\n",
      "epoch=280, train_loss=0.0307, val_loss=0.0112\n",
      "epoch=290, train_loss=0.0301, val_loss=0.0116\n",
      "epoch=300, train_loss=0.0301, val_loss=0.0115\n",
      "epoch=310, train_loss=0.0298, val_loss=0.0111\n",
      "epoch=320, train_loss=0.0294, val_loss=0.0111\n",
      "epoch=330, train_loss=0.0296, val_loss=0.0115\n",
      "epoch=340, train_loss=0.0291, val_loss=0.0112\n",
      "epoch=350, train_loss=0.0291, val_loss=0.0108\n",
      "epoch=360, train_loss=0.0290, val_loss=0.0109\n",
      "epoch=370, train_loss=0.0289, val_loss=0.0118\n",
      "epoch=380, train_loss=0.0288, val_loss=0.0112\n",
      "epoch=390, train_loss=0.0278, val_loss=0.0113\n",
      "epoch=400, train_loss=0.0288, val_loss=0.0112\n",
      "epoch=410, train_loss=0.0283, val_loss=0.0115\n",
      "epoch=420, train_loss=0.0284, val_loss=0.0114\n",
      "epoch=430, train_loss=0.0288, val_loss=0.0116\n",
      "epoch=440, train_loss=0.0284, val_loss=0.0112\n",
      "epoch=450, train_loss=0.0282, val_loss=0.0110\n",
      "epoch=460, train_loss=0.0278, val_loss=0.0113\n",
      "epoch=470, train_loss=0.0282, val_loss=0.0106\n",
      "epoch=480, train_loss=0.0278, val_loss=0.0115\n",
      "epoch=490, train_loss=0.0277, val_loss=0.0110\n",
      "epoch=500, train_loss=0.0277, val_loss=0.0111\n",
      "epoch=510, train_loss=0.0279, val_loss=0.0107\n",
      "epoch=520, train_loss=0.0280, val_loss=0.0115\n",
      "epoch=530, train_loss=0.0279, val_loss=0.0112\n",
      "epoch=540, train_loss=0.0277, val_loss=0.0113\n",
      "epoch=550, train_loss=0.0274, val_loss=0.0114\n",
      "epoch=560, train_loss=0.0281, val_loss=0.0105\n",
      "epoch=570, train_loss=0.0279, val_loss=0.0113\n",
      "epoch=580, train_loss=0.0283, val_loss=0.0104\n",
      "epoch=590, train_loss=0.0278, val_loss=0.0110\n",
      "epoch=600, train_loss=0.0279, val_loss=0.0106\n",
      "epoch=610, train_loss=0.0277, val_loss=0.0114\n",
      "epoch=620, train_loss=0.0276, val_loss=0.0107\n",
      "epoch=630, train_loss=0.0279, val_loss=0.0108\n",
      "epoch=640, train_loss=0.0276, val_loss=0.0114\n",
      "epoch=650, train_loss=0.0275, val_loss=0.0118\n",
      "epoch=660, train_loss=0.0277, val_loss=0.0111\n",
      "epoch=670, train_loss=0.0275, val_loss=0.0114\n",
      "epoch=680, train_loss=0.0280, val_loss=0.0101\n",
      "epoch=690, train_loss=0.0280, val_loss=0.0111\n",
      "epoch=700, train_loss=0.0277, val_loss=0.0110\n",
      "epoch=710, train_loss=0.0277, val_loss=0.0107\n",
      "epoch=720, train_loss=0.0273, val_loss=0.0113\n",
      "epoch=730, train_loss=0.0279, val_loss=0.0109\n",
      "epoch=740, train_loss=0.0276, val_loss=0.0116\n",
      "epoch=750, train_loss=0.0275, val_loss=0.0115\n",
      "epoch=760, train_loss=0.0272, val_loss=0.0111\n",
      "epoch=770, train_loss=0.0278, val_loss=0.0106\n",
      "epoch=780, train_loss=0.0278, val_loss=0.0111\n",
      "epoch=790, train_loss=0.0276, val_loss=0.0107\n",
      "epoch=800, train_loss=0.0277, val_loss=0.0111\n",
      "epoch=810, train_loss=0.0276, val_loss=0.0112\n",
      "epoch=820, train_loss=0.0274, val_loss=0.0112\n",
      "epoch=830, train_loss=0.0276, val_loss=0.0107\n",
      "epoch=840, train_loss=0.0271, val_loss=0.0116\n",
      "epoch=850, train_loss=0.0274, val_loss=0.0105\n",
      "epoch=860, train_loss=0.0274, val_loss=0.0105\n",
      "epoch=870, train_loss=0.0273, val_loss=0.0113\n",
      "epoch=880, train_loss=0.0274, val_loss=0.0113\n",
      "epoch=890, train_loss=0.0279, val_loss=0.0111\n",
      "epoch=900, train_loss=0.0275, val_loss=0.0110\n",
      "epoch=910, train_loss=0.0277, val_loss=0.0115\n",
      "epoch=920, train_loss=0.0281, val_loss=0.0112\n",
      "epoch=930, train_loss=0.0276, val_loss=0.0109\n",
      "epoch=940, train_loss=0.0274, val_loss=0.0111\n",
      "epoch=950, train_loss=0.0269, val_loss=0.0110\n",
      "epoch=960, train_loss=0.0276, val_loss=0.0107\n",
      "epoch=970, train_loss=0.0275, val_loss=0.0114\n",
      "epoch=980, train_loss=0.0270, val_loss=0.0112\n",
      "epoch=990, train_loss=0.0277, val_loss=0.0113\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "batch_size = 16384\n",
    "num_epochs = 1000\n",
    "optimizer = AdamW(model.parameters(), lr=0.2)\n",
    "\n",
    "scheduler = lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=0.99,\n",
    ")\n",
    "masker = RandomMasker(t)\n",
    "p_masker = 0.2\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # get into training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_num in range(X_train.shape[0] // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # initialize proper `x`\n",
    "        ix = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "        y = X_train[ix].long().to(\"cuda\")  # true values\n",
    "        x = masker.add_mask(y, p=p_masker).to(\"cuda\")  # noisy values\n",
    "\n",
    "        # predict demasked tokens\n",
    "        y_pred = model(x).swapaxes(-1, -2).to(\"cuda\")\n",
    "\n",
    "        # let's focus our loss on masked tokens only\n",
    "        is_masked = y_pred == t.stoi[t.mask_token]\n",
    "\n",
    "        true_labels = (\n",
    "            F.one_hot(y, num_classes=vocab_size).swapaxes(-1, -2).float().to(\"cuda\")\n",
    "        )\n",
    "        true_labels[~is_masked] = y_pred[~is_masked]\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "\n",
    "        # do backprop\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # validation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test = X_test.long().to(\"cuda\")\n",
    "        x_test = masker.add_mask(y_test, p=p_masker).to(\"cuda\")\n",
    "\n",
    "        y_pred_test = model(x_test).swapaxes(-1, -2).to(\"cuda\")\n",
    "        test_loss = F.cross_entropy(y_pred_test, y_test)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"{epoch=}, train_loss={torch.tensor(losses).mean().item():2.4f}, val_loss={test_loss.item():2.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 56, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 56, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 56, 16])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384, 56, 16])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (pe): PositionalEncoding()\n",
       "  (emb): Embedding(56, 128)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): LayerNorm((16, 64), eps=1e-05, elementwise_affine=True)\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.4, inplace=False)\n",
       "    (7): LayerNorm((16, 64), eps=1e-05, elementwise_affine=True)\n",
       "    (8): Linear(in_features=64, out_features=56, bias=True)\n",
       "    (9): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Janeng',\n",
       " 'Vovchynets',\n",
       " 'Melton',\n",
       " 'Zhirnovsk',\n",
       " 'Fahren',\n",
       " 'Schiesheim',\n",
       " 'Hodonin',\n",
       " 'Bellevue',\n",
       " 'Buenavista',\n",
       " 'Zhuangshi']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode(X_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ja#eng',\n",
       " 'Vovchynet#',\n",
       " 'Melton',\n",
       " 'Zhirno#sk',\n",
       " 'Fahr#n',\n",
       " 'Schiesheim',\n",
       " 'Hodonin',\n",
       " 'Bellevue',\n",
       " 'Bu#navist#',\n",
       " 'Zh#angshi']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_X_val = masker.add_mask(X_val, p=0.05)\n",
    "t.decode(masked_X_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ja#eng',\n",
       " 'Vovchynet#',\n",
       " 'Melton',\n",
       " 'Zhirno#sk',\n",
       " 'Fahr#n',\n",
       " 'Schiesheim',\n",
       " 'Hodonin',\n",
       " 'Bellevue',\n",
       " 'Bu#navist#',\n",
       " 'Zh#angshi']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode(model(masked_X_val.long().to(\"cuda\")).argmax(axis=-1))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
