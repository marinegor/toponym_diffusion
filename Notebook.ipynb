{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import GeonamesDataset\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geonames = GeonamesDataset(\"./data/cities500.txt.gz\", max_len=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sequence</th><th>feature code</th><th>country code</th><th>population</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Yamunanagar&quot;</td><td>&quot;PPL&quot;</td><td>&quot;IN&quot;</td><td>208931</td></tr><tr><td>&quot;Vredefort&quot;</td><td>&quot;PPL&quot;</td><td>&quot;ZA&quot;</td><td>14619</td></tr><tr><td>&quot;Hilbesheim&quot;</td><td>&quot;PPL&quot;</td><td>&quot;FR&quot;</td><td>529</td></tr><tr><td>&quot;Mapastepec&quot;</td><td>&quot;PPL&quot;</td><td>&quot;MX&quot;</td><td>17931</td></tr><tr><td>&quot;Holmsund&quot;</td><td>&quot;PPL&quot;</td><td>&quot;SE&quot;</td><td>5962</td></tr><tr><td>&quot;Strassoldo&quot;</td><td>&quot;PPL&quot;</td><td>&quot;IT&quot;</td><td>707</td></tr><tr><td>&quot;Tumannyy&quot;</td><td>&quot;PPL&quot;</td><td>&quot;RU&quot;</td><td>896</td></tr><tr><td>&quot;Ticaco&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;PE&quot;</td><td>741</td></tr><tr><td>&quot;Gjergjan&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;AL&quot;</td><td>0</td></tr><tr><td>&quot;Collesalvetti&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;IT&quot;</td><td>3530</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌───────────────┬──────────────┬──────────────┬────────────┐\n",
       "│ sequence      ┆ feature code ┆ country code ┆ population │\n",
       "│ ---           ┆ ---          ┆ ---          ┆ ---        │\n",
       "│ str           ┆ str          ┆ str          ┆ i64        │\n",
       "╞═══════════════╪══════════════╪══════════════╪════════════╡\n",
       "│ Yamunanagar   ┆ PPL          ┆ IN           ┆ 208931     │\n",
       "│ Vredefort     ┆ PPL          ┆ ZA           ┆ 14619      │\n",
       "│ Hilbesheim    ┆ PPL          ┆ FR           ┆ 529        │\n",
       "│ Mapastepec    ┆ PPL          ┆ MX           ┆ 17931      │\n",
       "│ Holmsund      ┆ PPL          ┆ SE           ┆ 5962       │\n",
       "│ Strassoldo    ┆ PPL          ┆ IT           ┆ 707        │\n",
       "│ Tumannyy      ┆ PPL          ┆ RU           ┆ 896        │\n",
       "│ Ticaco        ┆ PPLA3        ┆ PE           ┆ 741        │\n",
       "│ Gjergjan      ┆ PPLA3        ┆ AL           ┆ 0          │\n",
       "│ Collesalvetti ┆ PPLA3        ┆ IT           ┆ 3530       │\n",
       "└───────────────┴──────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geonames.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geonames.df\n",
    "alphabet = \"\".join(\n",
    "    set(\"\".join(df.get_column(\"sequence\").str.split(\"\").explode().to_list()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Tokenizer\n",
    "\n",
    "t = Tokenizer(\n",
    "    alphabet=alphabet,\n",
    "    max_len=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: torch.Size([101200, 16])\n",
      "Test set size: torch.Size([12650, 16])\n",
      "Validation set size: torch.Size([12651, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total_samples = X.size(0)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_train = int(total_samples * train_ratio)\n",
    "num_test = int(total_samples * test_ratio)\n",
    "num_val = total_samples - num_train - num_test\n",
    "\n",
    "# Generate random indices\n",
    "indices = torch.randperm(total_samples)\n",
    "\n",
    "# Split the indices into train, test, and validation sets\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train : num_train + num_test]\n",
    "val_indices = indices[num_train + num_test :]\n",
    "\n",
    "# Create the train, test, and validation sets\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "X_val = X[val_indices]\n",
    "\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Validation set size:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train = TensorDataset(X_train)\n",
    "test = TensorDataset(X_test)\n",
    "val = TensorDataset(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(DataLoader(train, batch_size=64)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[0].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_output: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_output = d_output\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = self.pe[t].squeeze(-1)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "class MLPWithTime(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_embed: nn.Module,\n",
    "        time_embed: nn.Module,\n",
    "        hidden: nn.Module,\n",
    "        output: nn.Module,\n",
    "        alphabet_size: int,\n",
    "        n_tokens: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed: nn.Module = token_embed\n",
    "        self.time_embed: nn.Module = time_embed\n",
    "        self.hidden: nn.Module = hidden\n",
    "        self.output: nn.Module = output\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.n_tokens = n_tokens\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        te = self.token_embed(x)\n",
    "        pe = self.time_embed(t)\n",
    "        he = self.hidden(te + pe)\n",
    "        output = self.output(he)\n",
    "        return output.reshape(\n",
    "            -1,\n",
    "            self.alphabet_size,\n",
    "            self.n_tokens,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dim = 16\n",
    "word_embeding_dim = time_embeding_dim = 128\n",
    "intermediate_dim = 64\n",
    "dropout = 0.4\n",
    "max_T = 100\n",
    "alphabet_size = len(t.stoi)\n",
    "\n",
    "Input = nn.Sequential(\n",
    "    nn.Linear(token_dim, word_embeding_dim),\n",
    "    nn.BatchNorm1d(word_embeding_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "TimeInput = PositionalEncoding(\n",
    "    d_model=1,\n",
    "    d_output=time_embeding_dim,\n",
    "    max_len=max_T,\n",
    ")\n",
    "Hidden = nn.Sequential(\n",
    "    nn.Linear(word_embeding_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.BatchNorm1d(intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "Output = nn.Sequential(\n",
    "    nn.Linear(intermediate_dim, alphabet_size * token_dim),\n",
    "    nn.BatchNorm1d(alphabet_size * token_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "\n",
    "model = MLPWithTime(\n",
    "    time_embed=TimeInput,\n",
    "    token_embed=Input,\n",
    "    hidden=Hidden,\n",
    "    output=Output,\n",
    "    alphabet_size=alphabet_size,\n",
    "    n_tokens=token_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = torch.randint(0, max_T, size=(x.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 55, 16])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(x, time).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78352"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import ForwardDiffusionProcess, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = Scheduler(T=max_T)\n",
    "diffusion = ForwardDiffusionProcess(scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26fbd4c506c4f2eb6494fcd54de4612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, torch.tensor(losses).mean()=tensor(2.8706)\n",
      "epoch=1, torch.tensor(losses).mean()=tensor(2.7818)\n",
      "epoch=2, torch.tensor(losses).mean()=tensor(2.7762)\n",
      "epoch=3, torch.tensor(losses).mean()=tensor(2.7811)\n",
      "epoch=4, torch.tensor(losses).mean()=tensor(2.7800)\n",
      "epoch=5, torch.tensor(losses).mean()=tensor(2.7795)\n",
      "epoch=6, torch.tensor(losses).mean()=tensor(2.7802)\n",
      "epoch=7, torch.tensor(losses).mean()=tensor(2.7780)\n",
      "epoch=8, torch.tensor(losses).mean()=tensor(2.7836)\n",
      "epoch=9, torch.tensor(losses).mean()=tensor(2.7824)\n",
      "epoch=10, torch.tensor(losses).mean()=tensor(2.7852)\n",
      "epoch=11, torch.tensor(losses).mean()=tensor(2.7879)\n",
      "epoch=12, torch.tensor(losses).mean()=tensor(2.7921)\n",
      "epoch=13, torch.tensor(losses).mean()=tensor(2.7861)\n",
      "epoch=14, torch.tensor(losses).mean()=tensor(2.7944)\n",
      "epoch=15, torch.tensor(losses).mean()=tensor(2.7963)\n",
      "epoch=16, torch.tensor(losses).mean()=tensor(2.8121)\n",
      "epoch=17, torch.tensor(losses).mean()=tensor(2.8085)\n",
      "epoch=18, torch.tensor(losses).mean()=tensor(2.8127)\n",
      "epoch=19, torch.tensor(losses).mean()=tensor(2.8049)\n",
      "epoch=20, torch.tensor(losses).mean()=tensor(2.8079)\n",
      "epoch=21, torch.tensor(losses).mean()=tensor(2.8043)\n",
      "epoch=22, torch.tensor(losses).mean()=tensor(2.8107)\n",
      "epoch=23, torch.tensor(losses).mean()=tensor(2.8105)\n",
      "epoch=24, torch.tensor(losses).mean()=tensor(2.8184)\n",
      "epoch=25, torch.tensor(losses).mean()=tensor(2.8190)\n",
      "epoch=26, torch.tensor(losses).mean()=tensor(2.8197)\n",
      "epoch=27, torch.tensor(losses).mean()=tensor(2.8173)\n",
      "epoch=28, torch.tensor(losses).mean()=tensor(2.8148)\n",
      "epoch=29, torch.tensor(losses).mean()=tensor(2.8175)\n",
      "epoch=30, torch.tensor(losses).mean()=tensor(2.8164)\n",
      "epoch=31, torch.tensor(losses).mean()=tensor(2.8230)\n",
      "epoch=32, torch.tensor(losses).mean()=tensor(2.8209)\n",
      "epoch=33, torch.tensor(losses).mean()=tensor(2.8236)\n",
      "epoch=34, torch.tensor(losses).mean()=tensor(2.8222)\n",
      "epoch=35, torch.tensor(losses).mean()=tensor(2.8226)\n",
      "epoch=36, torch.tensor(losses).mean()=tensor(2.8215)\n",
      "epoch=37, torch.tensor(losses).mean()=tensor(2.8278)\n",
      "epoch=38, torch.tensor(losses).mean()=tensor(2.8299)\n",
      "epoch=39, torch.tensor(losses).mean()=tensor(2.8275)\n",
      "epoch=40, torch.tensor(losses).mean()=tensor(2.8312)\n",
      "epoch=41, torch.tensor(losses).mean()=tensor(2.8325)\n",
      "epoch=42, torch.tensor(losses).mean()=tensor(2.8335)\n",
      "epoch=43, torch.tensor(losses).mean()=tensor(2.8346)\n",
      "epoch=44, torch.tensor(losses).mean()=tensor(2.8329)\n",
      "epoch=45, torch.tensor(losses).mean()=tensor(2.8370)\n",
      "epoch=46, torch.tensor(losses).mean()=tensor(2.8384)\n",
      "epoch=47, torch.tensor(losses).mean()=tensor(2.8313)\n",
      "epoch=48, torch.tensor(losses).mean()=tensor(2.8339)\n",
      "epoch=49, torch.tensor(losses).mean()=tensor(2.8311)\n",
      "epoch=50, torch.tensor(losses).mean()=tensor(2.8358)\n",
      "epoch=51, torch.tensor(losses).mean()=tensor(2.8371)\n",
      "epoch=52, torch.tensor(losses).mean()=tensor(2.8379)\n",
      "epoch=53, torch.tensor(losses).mean()=tensor(2.8356)\n",
      "epoch=54, torch.tensor(losses).mean()=tensor(2.8418)\n",
      "epoch=55, torch.tensor(losses).mean()=tensor(2.8384)\n",
      "epoch=56, torch.tensor(losses).mean()=tensor(2.8420)\n",
      "epoch=57, torch.tensor(losses).mean()=tensor(2.8377)\n",
      "epoch=58, torch.tensor(losses).mean()=tensor(2.8385)\n",
      "epoch=59, torch.tensor(losses).mean()=tensor(2.8392)\n",
      "epoch=60, torch.tensor(losses).mean()=tensor(2.8407)\n",
      "epoch=61, torch.tensor(losses).mean()=tensor(2.8519)\n",
      "epoch=62, torch.tensor(losses).mean()=tensor(2.8467)\n",
      "epoch=63, torch.tensor(losses).mean()=tensor(2.8508)\n",
      "epoch=64, torch.tensor(losses).mean()=tensor(2.8475)\n",
      "epoch=65, torch.tensor(losses).mean()=tensor(2.8518)\n",
      "epoch=66, torch.tensor(losses).mean()=tensor(2.8501)\n",
      "epoch=67, torch.tensor(losses).mean()=tensor(2.8487)\n",
      "epoch=68, torch.tensor(losses).mean()=tensor(2.8518)\n",
      "epoch=69, torch.tensor(losses).mean()=tensor(2.8535)\n",
      "epoch=70, torch.tensor(losses).mean()=tensor(2.8536)\n",
      "epoch=71, torch.tensor(losses).mean()=tensor(2.8566)\n",
      "epoch=72, torch.tensor(losses).mean()=tensor(2.8517)\n",
      "epoch=73, torch.tensor(losses).mean()=tensor(2.8512)\n",
      "epoch=74, torch.tensor(losses).mean()=tensor(2.8518)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "optimizer = AdamW(model.parameters(), lr=1)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    losses = []\n",
    "    for batch_num in range(X_train.shape[0] // batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ix = torch.randint(0, X_train.shape[0], size=(batch_size,))\n",
    "        batch = X_train[ix].long()\n",
    "        time = torch.randint(low=0, high=max_T, size=(batch.shape[0],))\n",
    "\n",
    "        x_0 = batch.float()\n",
    "        x_t = diffusion.sample_T(x_0, time)\n",
    "        x_0_pred = model(x_t, time)\n",
    "\n",
    "        loss = F.cross_entropy(x_0_pred, batch)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"{epoch=}, {torch.tensor(losses).mean().item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
