{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import GeonamesDataset\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "geonames = GeonamesDataset(\"./data/cities500.txt.gz\", max_len=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sequence</th><th>feature code</th><th>country code</th><th>population</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Heidelberg&quot;</td><td>&quot;PPLX&quot;</td><td>&quot;AU&quot;</td><td>7360</td></tr><tr><td>&quot;Antagnod&quot;</td><td>&quot;PPLA3&quot;</td><td>&quot;IT&quot;</td><td>242</td></tr><tr><td>&quot;Dunshang&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;CN&quot;</td><td>0</td></tr><tr><td>&quot;Taiping&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;CN&quot;</td><td>0</td></tr><tr><td>&quot;Schela&quot;</td><td>&quot;PPL&quot;</td><td>&quot;RO&quot;</td><td>590</td></tr><tr><td>&quot;Apastepeque&quot;</td><td>&quot;PPL&quot;</td><td>&quot;SV&quot;</td><td>5785</td></tr><tr><td>&quot;Monsireigne&quot;</td><td>&quot;PPL&quot;</td><td>&quot;FR&quot;</td><td>795</td></tr><tr><td>&quot;Wonfurt&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;DE&quot;</td><td>1955</td></tr><tr><td>&quot;Newstead&quot;</td><td>&quot;PPLX&quot;</td><td>&quot;AU&quot;</td><td>4719</td></tr><tr><td>&quot;Mondoteko&quot;</td><td>&quot;PPLA4&quot;</td><td>&quot;ID&quot;</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 4)\n",
       "┌─────────────┬──────────────┬──────────────┬────────────┐\n",
       "│ sequence    ┆ feature code ┆ country code ┆ population │\n",
       "│ ---         ┆ ---          ┆ ---          ┆ ---        │\n",
       "│ str         ┆ str          ┆ str          ┆ i64        │\n",
       "╞═════════════╪══════════════╪══════════════╪════════════╡\n",
       "│ Heidelberg  ┆ PPLX         ┆ AU           ┆ 7360       │\n",
       "│ Antagnod    ┆ PPLA3        ┆ IT           ┆ 242        │\n",
       "│ Dunshang    ┆ PPLA4        ┆ CN           ┆ 0          │\n",
       "│ Taiping     ┆ PPLA4        ┆ CN           ┆ 0          │\n",
       "│ Schela      ┆ PPL          ┆ RO           ┆ 590        │\n",
       "│ Apastepeque ┆ PPL          ┆ SV           ┆ 5785       │\n",
       "│ Monsireigne ┆ PPL          ┆ FR           ┆ 795        │\n",
       "│ Wonfurt     ┆ PPLA4        ┆ DE           ┆ 1955       │\n",
       "│ Newstead    ┆ PPLX         ┆ AU           ┆ 4719       │\n",
       "│ Mondoteko   ┆ PPLA4        ┆ ID           ┆ 0          │\n",
       "└─────────────┴──────────────┴──────────────┴────────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geonames.df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geonames.df\n",
    "alphabet = \"\".join(\n",
    "    set(\"\".join(df.get_column(\"sequence\").str.split(\"\").explode().to_list()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Tokenizer\n",
    "\n",
    "t = Tokenizer(\n",
    "    alphabet=alphabet,\n",
    "    max_len=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t.encode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: torch.Size([101200, 16])\n",
      "Test set size: torch.Size([12650, 16])\n",
      "Validation set size: torch.Size([12651, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total_samples = X.size(0)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "num_train = int(total_samples * train_ratio)\n",
    "num_test = int(total_samples * test_ratio)\n",
    "num_val = total_samples - num_train - num_test\n",
    "\n",
    "# Generate random indices\n",
    "indices = torch.randperm(total_samples)\n",
    "\n",
    "# Split the indices into train, test, and validation sets\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train : num_train + num_test]\n",
    "val_indices = indices[num_train + num_test :]\n",
    "\n",
    "# Create the train, test, and validation sets\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "X_val = X[val_indices]\n",
    "\n",
    "print(\"Train set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "print(\"Validation set size:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train = TensorDataset(X_train)\n",
    "test = TensorDataset(X_test)\n",
    "val = TensorDataset(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(DataLoader(train, batch_size=64)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[0].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_output: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_output = d_output\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = self.pe[: x.size(0)].flatten().unsqueeze(-1)\n",
    "        x = x.repeat(\n",
    "            1,\n",
    "            self.d_output,\n",
    "        )\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithTime(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_embed: nn.Module,\n",
    "        time_embed: nn.Module,\n",
    "        hidden: nn.Module,\n",
    "        output: nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_embed: nn.Module = token_embed\n",
    "        self.time_embed: nn.Module = time_embed\n",
    "        self.hidden: nn.Module = hidden\n",
    "        self.output: nn.Module = output\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        return self.output(self.hidden(self.token_embed(x) + self.time_embed(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dim = 16\n",
    "word_embeding_dim = time_embeding_dim = 128\n",
    "intermediate_dim = 32\n",
    "dropout = 0.3\n",
    "max_T = 100\n",
    "\n",
    "Input = nn.Sequential(\n",
    "    nn.Linear(token_dim, word_embeding_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "TimeInput = PositionalEncoding(\n",
    "    d_model=1,\n",
    "    d_output=time_embeding_dim,\n",
    "    max_len=max_T,\n",
    ")\n",
    "Hidden = nn.Sequential(\n",
    "    nn.Linear(word_embeding_dim, intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(intermediate_dim, intermediate_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    ")\n",
    "Output = nn.Sequential(\n",
    "    nn.Linear(intermediate_dim, word_embeding_dim),\n",
    "    nn.Softmax(dim=0),\n",
    ")\n",
    "\n",
    "model = MLPWithTime(\n",
    "    time_embed=TimeInput,\n",
    "    token_embed=Input,\n",
    "    hidden=Hidden,\n",
    "    output=Output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randint(0, max_T + 1, size=(x.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0061, 0.0159, 0.0159,  ..., 0.0122, 0.0218, 0.0234],\n",
       "        [0.0381, 0.0244, 0.0093,  ..., 0.0098, 0.0127, 0.0131],\n",
       "        [0.0175, 0.0066, 0.0223,  ..., 0.0239, 0.0097, 0.0118],\n",
       "        ...,\n",
       "        [0.0095, 0.0159, 0.0075,  ..., 0.0103, 0.0121, 0.0091],\n",
       "        [0.0096, 0.0032, 0.0071,  ..., 0.0066, 0.0242, 0.0058],\n",
       "        [0.0203, 0.0081, 0.0180,  ..., 0.0060, 0.0274, 0.0078]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
